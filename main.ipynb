{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uqxn9qnWq5"
   },
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RwDC_DMnfKM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DOWNLOAD_URL = \"https://github.com/fatimaezzahra-creator/Projet-ML/raw/refs/heads/main/datasets/adult.tgz\"\n",
    "DATASET_PATH = \"datasets\"\n",
    "\n",
    "def fetch_data(data_url, data_path):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, \"adult.tgz\")\n",
    "    urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    tgz_file = tarfile.open(tgz_path)\n",
    "    tgz_file.extractall(path=data_path)\n",
    "    tgz_file.close()    \n",
    "\n",
    "fetch_data(DOWNLOAD_URL, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj0BnOwJANom"
   },
   "source": [
    "## A Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJs-0QGk5gl-",
    "outputId": "0bb0ca2f-70a0-40e7-9f84-96b80d8aa47e"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    csv_path = os.path.join(DATASET_PATH, \"adult.data\")\n",
    "    return pd.read_csv(csv_path, skipinitialspace=True)\n",
    "\n",
    "data = load_data()\n",
    "target_name = \"class\"\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "-pJUHnhW7zPN",
    "outputId": "33293215-f3ee-4fe0-be34-20bcfe293aa5"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "7WpOgQ_y8AF2",
    "outputId": "8bdbd133-1946-4733-a134-563371adacff"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"marital-status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "uClH-PDd8Sll",
    "outputId": "150fab24-1bad-4e56-ea96-c0a6aad4a42b"
   },
   "outputs": [],
   "source": [
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target_name].value_counts()\n",
    "data.drop('education', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the target class is NOT balanced, so to create our train and test sets we can use a StratifiedShuffleSplit that will not only shuffle the instances but also preserve the proportions in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_indexes, test_indexes in split.split(data, data[target_name]):\n",
    "    train_set = data.iloc[train_indexes]\n",
    "    test_set = data.iloc[test_indexes]\n",
    "\n",
    "print(\"Proportions in the original dataset:\", data[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the train set:\", train_set[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the test set:\", test_set[target_name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate target from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set.drop(target_name, axis=1)\n",
    "target = train_set[target_name].copy()\n",
    "\n",
    "numerical_features = data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation of Text and Categorical Data\n",
    "The feature `education` is the only one which implies some kind of order, so we can use an `OrdinalEncoder`.\n",
    "The features `workclass`, `marital-status`, `relationship`, `race` and `sex` can all be handled by a `OneHotEncoder`.\n",
    "The features `occupation` and `native-country` have very high cardinality. They will also be handled by a `OneHotEncoder` for now, but we will eventually find a better solution.\n",
    "\n",
    "As for numerical features, we will only use a `StandardScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of Text and Categorical Data\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"categorical_not_ordinal\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"numerical\", StandardScaler(), numerical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "In this section, we will train 5 different models, evaluate them and compare to find the best one. They are:\n",
    "* A LogisticRegression;\n",
    "* An SGDClassifier;\n",
    "* A RandomForestClassifier;\n",
    "* A GradientBoostingClassifier;\n",
    "* A KNeighboursClassifier;\n",
    "\n",
    "First let's build each pipeline, with our previously defined preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "LR_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "SGD_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "RF_clf = RandomForestClassifier(random_state=42)\n",
    "GB_clf = GradientBoostingClassifier(random_state=42)\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "\n",
    "LR_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LR_clf)\n",
    "])\n",
    "SGD_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGD_clf)\n",
    "])\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RF_clf)\n",
    "])\n",
    "GB_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GB_clf)\n",
    "])\n",
    "KNN_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNN_clf)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic': LR_pipeline,\n",
    "    'SGD': SGD_pipeline,\n",
    "    'RandomForet': RF_pipeline,\n",
    "    'GradientBoosting': GB_pipeline,\n",
    "    'KNeighbours': KNN_pipeline \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the parameter grid for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_param_grid = {\n",
    "    'classifier__penalty': ['l1','l2'],\n",
    "    'classifier__C': [0.01, 0.1, 1],\n",
    "    'classifier__max_iter' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "SGD_param_grid = {\n",
    "        'classifier__learning_rate': ['constant', 'invscaling'],\n",
    "        'classifier__eta0': [0.01, 0.1, 1],\n",
    "        'classifier__penalty': ['l2', 'l1'],\n",
    "        'classifier__alpha': [0.001, 0.01],\n",
    "        'classifier__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [10, 50, 100]\n",
    "}\n",
    "\n",
    "GB_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'classifier__p': [1, 2] \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'Logistic': LR_param_grid,\n",
    "    'SGD': SGD_param_grid,\n",
    "    'RandomForet': RF_param_grid,\n",
    "    'GradientBoosting': GB_param_grid,\n",
    "    'KNeighbours': KNN_param_grid \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform a GridSearchCV. Since the parameter grid are already pretty extensive, we will use only 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"For the {name} model:\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid[name],\n",
    "        cv=5,                   # Validação cruzada 5-fold\n",
    "        scoring='accuracy',     # Métrica de avaliação\n",
    "        n_jobs=-1,              # Uso de todos os núcleos disponíveis\n",
    "    )\n",
    "    grid_search.fit(data, target)\n",
    "    print(\"    Best Hyperparams:\", grid_search.best_params_)\n",
    "    print(\"    Best Accuracy:\", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
