{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uqxn9qnWq5"
   },
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3RwDC_DMnfKM"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DOWNLOAD_URL = \"https://github.com/fatimaezzahra-creator/Projet-ML/raw/refs/heads/main/datasets/adult.tgz\"\n",
    "DATASET_PATH = \"datasets\"\n",
    "\n",
    "def fetch_data(data_url, data_path):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, \"adult.tgz\")\n",
    "    urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    tgz_file = tarfile.open(tgz_path)\n",
    "    tgz_file.extractall(path=data_path)\n",
    "    tgz_file.close()    \n",
    "\n",
    "fetch_data(DOWNLOAD_URL, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def load_data():\n",
    "    csv_path = os.path.join(DATASET_PATH, \"adult.data\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "data = load_data()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Missing Data Visualization\n",
    "sns.heatmap(data.isna(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the graph is all dark, which means there is no missing values in the data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Distribution Analysis\n",
    "data[\" class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Numerical attribute\n",
    "for col in data.select_dtypes(\"int64\"):\n",
    "    sns.displot(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical attribute\n",
    "import matplotlib.pyplot as plt\n",
    "for col in data.select_dtypes(\"object\"):\n",
    "    sns.displot(data=data, x=col)\n",
    "    plt.title(f\" '{col}'\", fontsize=16)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    plt.ylabel(\"Nombre d'observations\", fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of the data\n",
    "df=data.copy()\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Creating subsets based on the target variable\n",
    "class_0=df[df[\"class\"] == \"<=50K\"]\n",
    "class_1=df[df[\"class\"] ==\">50K\"]\n",
    "combined_df = (pd.concat([class_0, class_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-age\n",
    "import seaborn as sns\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"age\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-education_num\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"education-num\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ralation target-education\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"education\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = data.groupby(\" education\")[\" education-num\"].unique()\n",
    "# Afficher le mapping pour vérifier la correspondance\n",
    "for edu, edu_num in education_mapping.items():\n",
    "    print(f\"Education: {edu}, Education_Num: {edu_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-workclass\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"workclass\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-sex\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"sex\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-marital status\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"marital-status\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-relationship\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"relationship\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = data.groupby(\" marital-status\")[\" relationship\"].unique()\n",
    "\n",
    "# Afficher le mapping pour vérifier la correspondance\n",
    "for mrs, rshp in education_mapping.items():\n",
    "    print(f\"marital-status: {mrs}, relationship: {rshp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation taget-fnlwgt\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"fnlwgt\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation target-race\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"race\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-native country\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"native-country\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-occupation\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"occupation\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rlation target-capital gain\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"capital-gain\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ralation target-capitalloss\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"capital-loss\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capital Features Combination\n",
    "dff=df.copy()\n",
    "combined_df[\"capital_features\"] = dff[\"capital-gain\"] - combined_df[\"capital-loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target- hours per week\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"hours-per-week\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"bright\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target_name].value_counts()\n",
    "data.drop('education', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the target class is NOT balanced, so to create our train and test sets we can use a StratifiedShuffleSplit that will not only shuffle the instances but also preserve the proportions in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_indexes, test_indexes in split.split(data, data[target_name]):\n",
    "    train_set = data.iloc[train_indexes]\n",
    "    test_set = data.iloc[test_indexes]\n",
    "\n",
    "print(\"Proportions in the original dataset:\", data[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the train set:\", train_set[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the test set:\", test_set[target_name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate target from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set.drop(target_name, axis=1)\n",
    "target = train_set[target_name].copy()\n",
    "\n",
    "numerical_features = data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation of Text and Categorical Data\n",
    "The feature `education` is the only one which implies some kind of order, so we can use an `OrdinalEncoder`.\n",
    "The features `workclass`, `marital-status`, `relationship`, `race` and `sex` can all be handled by a `OneHotEncoder`.\n",
    "The features `occupation` and `native-country` have very high cardinality. They will also be handled by a `OneHotEncoder` for now, but we will eventually find a better solution.\n",
    "\n",
    "As for numerical features, we will only use a `StandardScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of Text and Categorical Data\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"categorical_not_ordinal\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"numerical\", StandardScaler(), numerical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "In this section, we will train 5 different models, evaluate them and compare to find the best one. They are:\n",
    "* A LogisticRegression;\n",
    "* An SGDClassifier;\n",
    "* A RandomForestClassifier;\n",
    "* A GradientBoostingClassifier;\n",
    "* A KNeighboursClassifier;\n",
    "\n",
    "First let's build each pipeline, with our previously defined preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "LR_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "SGD_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "RF_clf = RandomForestClassifier(random_state=42)\n",
    "GB_clf = GradientBoostingClassifier(random_state=42)\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "\n",
    "LR_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LR_clf)\n",
    "])\n",
    "SGD_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGD_clf)\n",
    "])\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RF_clf)\n",
    "])\n",
    "GB_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GB_clf)\n",
    "])\n",
    "KNN_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNN_clf)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic': LR_pipeline,\n",
    "    'SGD': SGD_pipeline,\n",
    "    'RandomForet': RF_pipeline,\n",
    "    'GradientBoosting': GB_pipeline,\n",
    "    'KNeighbours': KNN_pipeline \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the parameter grid for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_param_grid = {\n",
    "    'classifier__penalty': ['l1','l2'],\n",
    "    'classifier__C': [0.01, 0.1, 1],\n",
    "    'classifier__max_iter' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "SGD_param_grid = {\n",
    "        'classifier__learning_rate': ['constant', 'invscaling'],\n",
    "        'classifier__eta0': [0.01, 0.1, 1],\n",
    "        'classifier__penalty': ['l2', 'l1'],\n",
    "        'classifier__alpha': [0.001, 0.01],\n",
    "        'classifier__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [10, 50, 100]\n",
    "}\n",
    "\n",
    "GB_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'classifier__p': [1, 2] \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'Logistic': LR_param_grid,\n",
    "    'SGD': SGD_param_grid,\n",
    "    'RandomForet': RF_param_grid,\n",
    "    'GradientBoosting': GB_param_grid,\n",
    "    'KNeighbours': KNN_param_grid \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform a GridSearchCV. Since the parameter grid are already pretty extensive, we will use only 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"For the {name} model:\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid[name],\n",
    "        cv=5,                   # Validação cruzada 5-fold\n",
    "        scoring='accuracy',     # Métrica de avaliação\n",
    "        n_jobs=-1,              # Uso de todos os núcleos disponíveis\n",
    "    )\n",
    "    grid_search.fit(data, target)\n",
    "    print(\"    Best Hyperparams:\", grid_search.best_params_)\n",
    "    print(\"    Best Accuracy:\", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
