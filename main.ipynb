{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uqxn9qnWq5"
   },
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": null,
   "execution_count": null,
   "metadata": {
    "id": "3RwDC_DMnfKM"
   },
   "outputs": [],
   "outputs": [],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n",
      "File \u001b[1;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1465\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1507\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1308\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1945\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fezeu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\fezeu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DOWNLOAD_URL = \"https://github.com/fatimaezzahra-creator/Projet-ML/raw/refs/heads/main/datasets/adult.tgz\"\n",
    "DATASET_PATH = \"datasets\"\n",
    "\n",
    "def fetch_data(data_url, data_path):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, \"adult.tgz\")\n",
    "    urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    tgz_file = tarfile.open(tgz_path)\n",
    "    tgz_file.extractall(path=data_path)\n",
    "    tgz_file.close()    \n",
    "\n",
    "fetch_data(DOWNLOAD_URL, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def load_data():\n",
    "    csv_path = os.path.join(DATASET_PATH, \"adult.data\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "data = load_data()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Missing Data Visualization\n",
    "sns.heatmap(data.isna(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the graph is all dark, which means there is no missing values in the data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Distribution Analysis\n",
    "data[\" class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Numerical attribute\n",
    "for col in data.select_dtypes(\"int64\"):\n",
    "    sns.displot(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical attribute\n",
    "import matplotlib.pyplot as plt\n",
    "for col in data.select_dtypes(\"object\"):\n",
    "    sns.displot(data=data, x=col)\n",
    "    plt.title(f\" '{col}'\", fontsize=16)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    plt.ylabel(\"Nombre d'observations\", fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of the data\n",
    "df=data.copy()\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Creating subsets based on the target variable\n",
    "class_0 = df[df[\"class\"] == \"<=50K\"]\n",
    "class_1 = df[df[\"class\"] == \">50K\"]\n",
    "class_0 = df[df[\"class\"] == \"<=50K\"]\n",
    "class_1 = df[df[\"class\"] == \">50K\"]\n",
    "combined_df = (pd.concat([class_0, class_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-age\n",
    "import seaborn as sns\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"age\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-education_num\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"education-num\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ralation target-education\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"education\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features education (categorical) and education-num (numerical) may convey similar information, as they both represent the education level of an individual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = data.groupby(\" education\")[\" education-num\"].unique()\n",
    "# Afficher le mapping pour vérifier la correspondance\n",
    "for edu, edu_num in education_mapping.items():\n",
    "    print(f\"Education: {edu}, Education_Num: {edu_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this mapping shows that each education category corresponds to a single unique value of education-num. This confirms that the two features are effectively encoding the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-workclass\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"workclass\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-sex\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"sex\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-marital status\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"marital-status\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-relationship\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"relationship\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features relationship and marital-status  might convey similar information because a person's relationship type often depends on their marital status.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = data.groupby(\" marital-status\")[\" relationship\"].unique()\n",
    "\n",
    "# Afficher le mapping pour vérifier la correspondance\n",
    "for mrs, rshp in education_mapping.items():\n",
    "    print(f\"marital-status: {mrs}, relationship: {rshp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output reveals that for each value of marital-status, there are multiple possible values for relationship.\n",
    "This variability indicates that a person's relationship cannot be uniquely determined based on their marital-status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation taget-fnlwgt\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"fnlwgt\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation target-race\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"race\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-native country\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"native-country\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The native-country  and race columns in the dataset contains many unique values, some of which have very low frequencies. Keeping all these rare categories can negatively impact the machine learning model due to:\n",
    "\n",
    "Overfitting: The model may place undue importance on rare categories, learning patterns that don't generalize well to new data.\n",
    "Increased Complexity: High cardinality increases the dimensionality during encoding (e.g., in one-hot encoding), which can slow down training and complicate the model unnecessarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target-occupation\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"occupation\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rlation target-capital gain\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"capital-gain\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ralation target-capitalloss\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"capital-loss\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"muted\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The combination of capital-gain and capital-loss into a single derived feature could have a stronger correlation with the target variable (class) than either capital-gain or capital-loss individually, potentially improving the predictive power of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capital Features Combination\n",
    "dff=df.copy()\n",
    "combined_df[\"capital_features\"] = dff[\"capital-gain\"] - combined_df[\"capital-loss\"]\n",
    "#Capital Features Combination\n",
    "dff=df.copy()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#add the column of capital_feature to the data\n",
    "dff[\"capital_net\"]=(dff[\"capital-gain\"] -dff[\"capital-loss\"])\n",
    "dff[\"ratio\"]=(dff[\"capital-gain\"] /(dff[\"capital-loss\"]+0.00000001))\n",
    "dff[\"capital_weighted\"]=(dff[\"capital-gain\"]*0.223329 +dff[\"capital-loss\"]*0.150526)\n",
    "\n",
    "\n",
    "# Encodage LabelEncoder pour chaque colonne catégorique\n",
    "label_encoders = {}\n",
    "for col in dff.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    dff[col] = le.fit_transform(dff[col])\n",
    "    label_encoders[col] = le  # Stocker l'encodeur pour chaque colonne (optionnel, utile pour l'inverse_transform)\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "corr_matrix = dff.corr()\n",
    "corr_matrix[\"class\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**capital_net: the net difference between gains and losses**\n",
    "\n",
    "Correlation: 0.214, lower than capital-gain. Relevance: While intuitive (netting gains and losses), this feature does not add much value compared to capital-gain alone. Consider dropping it unless it improves model performance.\n",
    "\n",
    "**ratio: relative proportion of gains to losses**\n",
    "\n",
    "Correlation: 0.223, identical to capital-gain. Relevance: This feature does not improve upon capital-gain’s correlation. Its usefulness might depend on the model's capacity to interpret non-linear relationships, but it seems redundant for linear models.\n",
    "\n",
    "**capital_weighted: weighted sum of the two based on their importance**\n",
    "\n",
    "Correlation: 0.229, slightly higher than capital-gain (0.223). This feature combines the effects of both gains and losses, weighted by their individual correlations with class. It shows a slight improvement, suggesting it may capture some additional nuanced information. This feature is pertinent to keep for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation target- hours per week\n",
    "sns.histplot(\n",
    "  data=combined_df ,\n",
    "  x=\"hours-per-week\",\n",
    "  hue=\"class\",\n",
    "  stat=\"density\",\n",
    "  common_norm=False,\n",
    "  palette=\"bright\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**synthesis**\n",
    "\n",
    "Retain education-num (numerical feature) and remove education to avoid redundancy and simplify the dataset.\n",
    "\n",
    "Retain both features relationship and marital-status as they capture different aspects of an individual's social situation.\n",
    "\n",
    "Focus on the most impactful features:\n",
    "education-num, age, hours-per-week, capital_weighted, and categorical variables such as relationship and marital-status.\n",
    "\n",
    "fnlwgt  adds minimal value to the predictive power of the model.\n",
    "\n",
    "Reduce noise, we group all rare categories (those with a frequency below a certain threshold, e.g., 50 occurrences) into a single category called \"Other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `education` is the only one which implies some kind of order, so we can use an `OrdinalEncoder`.\n",
    "The features `workclass`, `marital-status`, `relationship`, `race` and `sex` can all be handled by a `OneHotEncoder`.\n",
    "The features `occupation` and `native-country` have very high cardinality. They will also be handled by a `OneHotEncoder` for now, but we will eventually find a better solution.\n",
    "\n",
    "As for numerical features, we will only use a `StandardScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of Text and Categorical Data\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ordinal_features = [\"education\"]\n",
    "not_ordinal_features = categorical_features.copy()\n",
    "not_ordinal_features.remove(\"education\")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"categorical_ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), ordinal_features),\n",
    "    (\"categorical_not_ordinal\", OneHotEncoder(handle_unknown=\"ignore\"), not_ordinal_features),\n",
    "    (\"numerical\", StandardScaler(), numerical_features)\n",
    "])\n",
    "\n",
    "data_prepared = preprocessor.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'class'\n",
    "combined_df = combined_df.drop('education', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the distribution of the target class is NOT balanced, so to create our train and test sets we can use a StratifiedShuffleSplit that will not only shuffle the instances but also preserve the proportions in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_indexes, test_indexes in split.split(combined_df, combined_df[target_name]):\n",
    "for train_indexes, test_indexes in split.split(combined_df, combined_df[target_name]):\n",
    "    train_set = data.iloc[train_indexes]\n",
    "    test_set = data.iloc[test_indexes]\n",
    "\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "train_set.columns = train_set.columns.str.strip()\n",
    "train_set = train_set.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "test_set.columns = test_set.columns.str.strip()\n",
    "test_set = test_set.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "print(\"Proportions in the original dataset:\", combined_df[target_name].value_counts(normalize=True))\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "train_set.columns = train_set.columns.str.strip()\n",
    "train_set = train_set.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Supprimer les espaces des noms de colonnes et des valeurs\n",
    "test_set.columns = test_set.columns.str.strip()\n",
    "test_set = test_set.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "print(\"Proportions in the original dataset:\", combined_df[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the train set:\", train_set[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the test set:\", test_set[target_name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate target from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_set.drop(target_name, axis=1)\n",
    "target = train_set[target_name].copy()\n",
    "\n",
    "numerical_features = train_data.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Low-Impact Features** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine capital features in capital_weighted\n",
    "df[\"capital_weighted\"] = df[\"capital-gain\"] * 0.223329 + df[\"capital-loss\"] * 0.150526\n",
    "\n",
    "# Delete the redundant columns\n",
    "df.drop([\"education\", \"fnlwgt\", \"capital-gain\", \"capital-loss\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping Rare Categories into 'Other' to Simplify Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of Text and Categorical Data\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Count the frequency of each category \n",
    "native_country_counts = df['native-country'].value_counts()\n",
    "race_counts = df['race'].value_counts()\n",
    "\n",
    "# Identify categories to keep \n",
    "to_keep = native_country_counts[native_country_counts >= 500].index\n",
    "to_keep_race = race_counts[race_counts >= 500].index\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"numerical\", StandardScaler(), numerical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "In this section, we will train 5 different models, evaluate them and compare to find the best one. They are:\n",
    "* A LogisticRegression;\n",
    "* An SGDClassifier;\n",
    "* A RandomForestClassifier;\n",
    "* A GradientBoostingClassifier;\n",
    "* A KNeighboursClassifier;\n",
    "\n",
    "First let's build each pipeline, with our previously defined preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "LR_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "SGD_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "RF_clf = RandomForestClassifier(random_state=42)\n",
    "GB_clf = GradientBoostingClassifier(random_state=42)\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "\n",
    "LR_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LR_clf)\n",
    "])\n",
    "SGD_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGD_clf)\n",
    "])\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RF_clf)\n",
    "])\n",
    "GB_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GB_clf)\n",
    "])\n",
    "KNN_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNN_clf)\n",
    "])\n",
    "# Replace rare categories with \"Other\"\n",
    "df['native-country'] = df['native-country'].apply(lambda x: x if x in to_keep else 'Other')\n",
    "df['race'] = df['race'].apply(lambda x: x if x in to_keep_race else 'Other')\n",
    "\n",
    "models = {\n",
    "    'Logistic': LR_pipeline,\n",
    "    'SGD': SGD_pipeline,\n",
    "    'RandomForet': RF_pipeline,\n",
    "    'GradientBoosting': GB_pipeline,\n",
    "    'KNeighbours': KNN_pipeline \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the parameter grid for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_param_grid = {\n",
    "    'classifier__penalty': ['l1','l2'],\n",
    "    'classifier__C': [0.01, 0.1, 1],\n",
    "    'classifier__max_iter' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "SGD_param_grid = {\n",
    "        'classifier__learning_rate': ['constant', 'invscaling'],\n",
    "        'classifier__eta0': [0.01, 0.1, 1],\n",
    "        'classifier__penalty': ['l2', 'l1'],\n",
    "        'classifier__alpha': [0.001, 0.01, 0.1],\n",
    "        'classifier__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [10, 50, 100]\n",
    "}\n",
    "\n",
    "GB_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'classifier__p': [1, 2] \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'Logistic': LR_param_grid,\n",
    "    'SGD': SGD_param_grid,\n",
    "    'RandomForet': RF_param_grid,\n",
    "    'GradientBoosting': GB_param_grid,\n",
    "    'KNeighbours': KNN_param_grid \n",
    "}"
    "# Verify the updated frequencies\n",
    "print(df['native-country'].value_counts())\n",
    "print(df['race'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform a GridSearchCV. Since the parameter grid are already pretty extensive, we will use only 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"For the {name} model:\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid[name],\n",
    "        cv=3,                   # Validação cruzada 5-fold\n",
    "        scoring='accuracy',     # Métrica de avaliação\n",
    "        n_jobs=-1,              # Uso de todos os núcleos disponíveis\n",
    "    )\n",
    "    grid_search.fit(train_data, target)\n",
    "    print(\"    Best Hyperparams:\", grid_search.best_params_)\n",
    "    print(\"    Best Accuracy:\", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
