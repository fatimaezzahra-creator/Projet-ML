{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Project Description\n",
    "The primary objective of this project is to apply key **Machine Learning** concepts, such as data cleaning, feature engineering, model selection, hyperparameter tuning, and performance evaluation. By exploring various  models and evaluation metrics, we expect to gain a deeper understanding of how to build and optimize predictive models.\n",
    "\n",
    "The aim is to perform a classification task using the well-known **Adult Income Dataset**. The target variable is whether a person earns more or less than 50K USD per year.\n",
    "\n",
    "## Dataset Description\n",
    "The [Adult Income Dataset](https://archive.ics.uci.edu/dataset/2/adult) was originally extracted from the 1994 US Census Database and is commonly used for binary classification tasks in ML. It contains demographic and economic features that may influence a person’s income level.\n",
    "\n",
    "It consists of 6 numerical features and 9 categorical features, including:\n",
    "* Demographic Information: `age`, `sex`, `race`, `marital-status`, `native-country`, `relationship`.\n",
    "* Education & Occupation: `education`, `education-num`, `workclass`, `occupation`.\n",
    "* Financial Attributes: `capital-gain`, `capital-loss`, `hours-per-week`, `fnlwgt`.\n",
    "* The Target Variable: `class` (<=50K or >50K)\n",
    "\n",
    "## Project Overview\n",
    "1. **Getting Started**: import libs and access the data.\n",
    "2. **Exploratory Data Analysis**: Explore the dataset, visualize feature distributions, identify correlations.\n",
    "3. **Data Preprocessing**: Handle missing values, handle text, scale data, split between train and test set.\n",
    "4. **Model Training**: Train different classification models and perform hyperparameter tuning.\n",
    "5. **Model Evaluation**: Evaluate model performance using various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "You can download all of the necessary packages using: `pip install pandas numpy seaborn matplotlib scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manipulation tools\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# Data visualization tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Predictors\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31uqxn9qnWq5"
   },
   "source": [
    "## Data Access\n",
    "The dataset is hosted on our GitHub repository. There is also a description from the original authors in `adult.names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RwDC_DMnfKM"
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = \"https://github.com/fatimaezzahra-creator/Projet-ML/raw/refs/heads/main/datasets/adult.tgz\"\n",
    "DATASET_PATH = \"datasets\"\n",
    "\n",
    "def fetch_data(data_url, data_path):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, \"adult.tgz\")\n",
    "    urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    tgz_file = tarfile.open(tgz_path)\n",
    "    tgz_file.extractall(path=data_path)\n",
    "    tgz_file.close()    \n",
    "\n",
    "def load_data():\n",
    "    csv_path = os.path.join(DATASET_PATH, \"adult.data\")\n",
    "    return pd.read_csv(csv_path, skipinitialspace=True)\n",
    "\n",
    "fetch_data(DOWNLOAD_URL, DATASET_PATH)\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Form and Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Missing Data\n",
    "sns.heatmap(data.isna(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the graph is all dark, which means there is no missing values in the data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the Target class\n",
    "target_name = \"class\"\n",
    "data[target_name].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Numerical Features\n",
    "for col in data.select_dtypes(\"int64\"):\n",
    "    sns.displot(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Categorical Attributes\n",
    "for col in data.select_dtypes(\"object\"):\n",
    "    sns.displot(data=data, x=col)\n",
    "    plt.title(f\" '{col}'\", fontsize=16)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    plt.ylabel(\"Nombre d'observations\", fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data\n",
    "df = data.copy()\n",
    "\n",
    "# Creating subsets based on the target variable\n",
    "class_0 = df[df[target_name] == \"<=50K\"]\n",
    "class_1 = df[df[target_name] == \">50K\"]\n",
    "combined_df = (pd.concat([class_0, class_1]))\n",
    "\n",
    "def visualize_correlation(dataframe, feature_name, target_name, rotate=False):\n",
    "    sns.histplot(\n",
    "        data=dataframe,\n",
    "        x=feature_name,\n",
    "        hue=target_name,\n",
    "        stat=\"density\",\n",
    "        common_norm=False,\n",
    "        palette=\"muted\")\n",
    "    if rotate:\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "def visualize_correspondance(dataframe, feature1_name, feature2_name):\n",
    "    # FIX ME: add comment explaining what does the line below do\n",
    "    mapping = dataframe.groupby(feature1_name)[feature2_name].unique()\n",
    "    # Show mapping to verify correspondance\n",
    "    for feature1, feature2 in mapping.items():\n",
    "        print(f\"{feature1_name}: {feature1}, {feature2_name}: {feature2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'age' feature\n",
    "visualize_correlation(combined_df, \"age\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'sex' feature\n",
    "visualize_correlation(combined_df, \"sex\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'workclass' feature\n",
    "visualize_correlation(combined_df, \"workclass\", target_name, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'fnlwgt' feature\n",
    "visualize_correlation(combined_df, \"fnlwgt\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'occupation' feature\n",
    "visualize_correlation(combined_df, \"occupation\", target_name, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'hours-per-week' feature\n",
    "visualize_correlation(combined_df, \"hours-per-week\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'education-num' feature\n",
    "visualize_correlation(combined_df, \"education-num\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'education' feature\n",
    "visualize_correlation(combined_df, \"education\", target_name, rotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `education` (categorical) and `education-num` (numerical) may convey similar information, as they both represent the education level of an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondance between 'education' and 'education-num'\n",
    "visualize_correspondance(data, \"education\", \"education-num\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this mapping shows that each education category corresponds to a single unique value of education-num in an ordinal way. That is, 1 is equivalent to the lowest level of education (Preschool), while 16 is the highest (Doctorate). This confirms that the two features are effectively encoding the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'marital-status' feature\n",
    "visualize_correlation(combined_df, \"marital-status\", target_name, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'relationship' feature\n",
    "visualize_correlation(combined_df, \"relationship\", target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `relationship` and `marital-status` might also convey similar information because a person's relationship type often depends on their marital status.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondance between 'marital-status' and 'relationship'\n",
    "visualize_correspondance(data, \"marital-status\", \"relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output reveals that for each value of marital-status, there are multiple possible values for relationship.\n",
    "This variability indicates that a person's relationship cannot be uniquely determined based on their marital-status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'race' feature\n",
    "visualize_correlation(combined_df, \"race\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'native-country' feature\n",
    "visualize_correlation(combined_df, \"native-country\", target_name, rotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The native-country  and race columns in the dataset contains many unique values, some of which have very low frequencies. Keeping all these rare categories can negatively impact the machine learning model due to:\n",
    "\n",
    "Overfitting: The model may place undue importance on rare categories, learning patterns that don't generalize well to new data.\n",
    "Increased Complexity: High cardinality increases the dimensionality during encoding (e.g., in one-hot encoding), which can slow down training and complicate the model unnecessarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'capital-gain' feature\n",
    "visualize_correlation(combined_df, \"capital-gain\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target and the 'capital-loss' feature\n",
    "visualize_correlation(combined_df, \"capital-loss\", target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of capital-gain and capital-loss into a single derived feature could have a stronger correlation with the target variable (class) than either capital-gain or capital-loss individually, potentially improving the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capital Features Combination\n",
    "dff = df.copy()\n",
    "dff[\"capital-net\"] = (dff[\"capital-gain\"] - dff[\"capital-loss\"])\n",
    "dff[\"ratio\"] = (dff[\"capital-gain\"]/(dff[\"capital-loss\"]+0.00000001))\n",
    "dff[\"capital-weighted\"] = (dff[\"capital-gain\"]*0.223329 + dff[\"capital-loss\"]*0.150526)\n",
    "\n",
    "# Encodage LabelEncoder pour chaque colonne catégorique\n",
    "label_encoders = {}\n",
    "for column in dff.select_dtypes(include=['object']).columns:\n",
    "    encoder = LabelEncoder()\n",
    "    dff[column] = encoder.fit_transform(dff[column])\n",
    "    label_encoders[column] = encoder  # Stocker l'encodeur pour chaque colonne (optionnel, utile pour l'inverse_transform)\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "corr_matrix = dff.corr()\n",
    "corr_matrix[target_name].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**capital-net: the net difference between gains and losses**\n",
    "\n",
    "Correlation: 0.214, lower than capital-gain. Relevance: While intuitive (netting gains and losses), this feature does not add much value compared to capital-gain alone. Consider dropping it unless it improves model performance.\n",
    "\n",
    "**ratio: relative proportion of gains to losses**\n",
    "\n",
    "Correlation: 0.223, identical to capital-gain. Relevance: This feature does not improve upon capital-gain’s correlation. Its usefulness might depend on the model's capacity to interpret non-linear relationships, but it seems redundant for linear models.\n",
    "\n",
    "**capital-weighted: weighted sum of the two based on their importance**\n",
    "\n",
    "Correlation: 0.229, slightly higher than capital-gain (0.223). This feature combines the effects of both gains and losses, weighted by their individual correlations with class. It shows a slight improvement, suggesting it may capture some additional nuanced information. This feature is pertinent to keep for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthesis**\n",
    "\n",
    "1. Retain `education-num` (numerical feature) and remove education to avoid redundancy and simplify the dataset.\n",
    "2. Retain both features `relationship` and `marital-status` as they capture different aspects of an individual's social situation.\n",
    "3. Focus on the most impactful features:\n",
    "`education-num`, `age`, `hours-per-week`, `capital-weighted`, and categorical variables such as `relationship` and `marital-status`.\n",
    "4. The feature `fnlwgt` adds minimal value to the predictive power of the model.\n",
    "5. To reduce noise, we can group all rare categories (those with a frequency below a certain threshold, e.g., 500 occurrences) into a single category called `\"Other\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Low-Impact Features** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine capital features in capital_weighted\n",
    "df[\"capital-weighted\"] = df[\"capital-gain\"] * 0.223329 + df[\"capital-loss\"] * 0.150526\n",
    "\n",
    "# Delete the redundant or irrelevant columns\n",
    "df.drop([\"education\", \"fnlwgt\", \"capital-gain\", \"capital-loss\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping Rare Categories into 'Other' to Simplify Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each category \n",
    "native_country_counts = df['native-country'].value_counts()\n",
    "race_counts = df['race'].value_counts()\n",
    "\n",
    "# Identify categories to keep \n",
    "to_keep = native_country_counts[native_country_counts >= 500].index\n",
    "to_keep_race = race_counts[race_counts >= 500].index\n",
    "\n",
    "# Replace rare categories with \"Other\"\n",
    "df['native-country'] = df['native-country'].apply(lambda x: x if x in to_keep else 'Other')\n",
    "df['race'] = df['race'].apply(lambda x: x if x in to_keep_race else 'Other')\n",
    "\n",
    "# Verify the updated frequencies\n",
    "print(df['native-country'].value_counts())\n",
    "print(df['race'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation of Text and Categorical Data\n",
    "numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_features.remove(target_name)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"numerical\", StandardScaler(), numerical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the distribution of the target class is NOT balanced, so to create our train and test sets we can use a StratifiedShuffleSplit that will not only shuffle the instances but also preserve the proportions in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_indexes, test_indexes in split.split(df, df[target_name]):\n",
    "    train_set = df.iloc[train_indexes]\n",
    "    test_set = df.iloc[test_indexes]\n",
    "\n",
    "print(\"Proportions in the original dataset:\", df[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the train set:\", train_set[target_name].value_counts(normalize=True))\n",
    "print(\"Proportions in the test set:\", test_set[target_name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can separate the target from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_set.drop(target_name, axis=1)\n",
    "target = train_set[target_name].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "In this section, we will train 5 different models, evaluate them and compare to find the best one. They are:\n",
    "* A LogisticRegression;\n",
    "* An SGDClassifier;\n",
    "* A RandomForestClassifier;\n",
    "* A GradientBoostingClassifier;\n",
    "* A KNeighboursClassifier;\n",
    "\n",
    "First let's build each pipeline, with our previously defined preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "SGD_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "RF_clf = RandomForestClassifier(random_state=42)\n",
    "GB_clf = GradientBoostingClassifier(random_state=42)\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "\n",
    "LR_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LR_clf)\n",
    "])\n",
    "SGD_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGD_clf)\n",
    "])\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RF_clf)\n",
    "])\n",
    "GB_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GB_clf)\n",
    "])\n",
    "KNN_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNN_clf)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic': LR_pipeline,\n",
    "    'SGD': SGD_pipeline,\n",
    "    'RandomForet': RF_pipeline,\n",
    "    'GradientBoosting': GB_pipeline,\n",
    "    'KNeighbours': KNN_pipeline \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the parameter grid for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_param_grid = {\n",
    "    'classifier__penalty': ['l1','l2'],\n",
    "    'classifier__C': [0.01, 0.1, 1],\n",
    "    'classifier__max_iter' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "SGD_param_grid = {\n",
    "    'classifier__learning_rate': ['constant', 'invscaling'],\n",
    "    'classifier__eta0': [0.01, 0.1, 1],\n",
    "    'classifier__penalty': ['l2', 'l1'],\n",
    "    'classifier__alpha': [0.001, 0.01, 0.1],\n",
    "    'classifier__max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [10, 50, 100]\n",
    "}\n",
    "\n",
    "GB_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'classifier__p': [1, 2] \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'Logistic': LR_param_grid,\n",
    "    'SGD': SGD_param_grid,\n",
    "    'RandomForet': RF_param_grid,\n",
    "    'GradientBoosting': GB_param_grid,\n",
    "    'KNeighbours': KNN_param_grid \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform a GridSearchCV. Since the parameter grid are already pretty extensive, we will use only 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"For the {name} model:\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid[name],\n",
    "        cv=3,                   # Validação cruzada 5-fold\n",
    "        scoring='accuracy',     # Métrica de avaliação\n",
    "        n_jobs=-1,              # Uso de todos os núcleos disponíveis\n",
    "    )\n",
    "    grid_search.fit(train_data, target)\n",
    "    print(\"    Best Hyperparams:\", grid_search.best_params_)\n",
    "    print(\"    Best Accuracy:\", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
